# lewidi2025

Start the vLLM server

In thinking mode:

```shell
vllm serve Qwen/Qwen3-32B \
    --dtype auto \
    --enable-reasoning \
    --reasoning-parser deepseek_r1 \
    --task generate \
    --disable-log-requests \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.95 \
    --enable-chunked-prefill
```

Use non-thinking mode, as describe in the [Qwen3 docs](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes):

```shell
vllm serve Qwen/Qwen3-32B \
    --dtype auto \
    --chat-template ./qwen3_nonthinking.jinja \
    --task generate \
    --disable-log-requests \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.95 \
    --enable-chunked-prefill
```

Start the server and run the inference:

```shell
python inference.py \
    --model_id Qwen/Qwen3-4B \
    --gen_kwargs nonthinking \
    --dataset MP \
    --n_examples 100 \
    --remote_call_concurrency 256 \
    --n_loops 3 \
    --vllm_port 8000 \
    --vllm_start_server=True
```

# SLURM
Create the sbatch files:

```bash
cd lewidi2025/slurm
python create_sbatch_files.py
```

submit all those jobs:

```bash
cd slurm_scripts/
ls | xargs -n 1 sbatch
```

Check the status of the jobs:

```bash
squeue -u $USER
```

## Plot Metrics

After installing the package, you can plot the metrics by running:

```bash
lewidi-plot --log_file /dss/dssfs02/lwp-dss-0001/pn76je/pn76je-dss-0000/lewidi-data/sbatch/di38bec/Qwen_Qwen3-32B_thinking/out.logs
```

Where `out.logs` is generated by the `sbatch` file.