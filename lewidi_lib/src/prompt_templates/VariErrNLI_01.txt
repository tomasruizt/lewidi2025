# Dataset Description:
The VariErr NLI dataset (VariErrNLI)
VariErrNLI was created designed for automatic error detection, distinguishing between annotation errors and legitimate human label variations in Natural Language Inference (NLI) tasks. The dataset was created using a two-round annotation process: initially, annotators provided labels and explanations for each NLI item; subsequently, they assessed the validity of each label-explanation pair. It comprises 1,933 explanations for 500 re-annotated items from the Multi-Genre Natural Language Inference (MNLI) corpus for Round 1 and 7,732 validity judgments for Round 2. The LeWiDi 2025 Shared Task focuses on Round 1, annotators could assign one or more labels from {{Entailment, Neutral, Contradiction}} to each (Premise, Hypothesis) pair and provide corresponding explanations.

# Task:
You are given a single example from the dataset, not the full dataset.
Your task is to estimate the fraction of annotators that selected each label for this example.
In other words, estimate the distribution of annotations for this example.

# Constraints:
Please output the fractions / probabilities that each label is chosen (entailment, neutral, contradiction) separately, where p1 is the probability that the label is chosen, and p0 is the probability that the label is not chosen. The sum p0 + p1 equals 1 for each label.

Output the fractions / probabilities in JSON format like below. 

{{ "entailment": {{ "0": p0, "1": p1 }}, "neutral": {{ "0": p0, "1": p1 }}, "contradiction": {{ "0": p0, "1": p1 }} }}

# Example:
{text}