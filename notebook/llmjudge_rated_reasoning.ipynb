{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lewidi_lib import (\n",
    "    assign_cols_perf_metrics,\n",
    "    enable_logging,\n",
    "    join_correct_responses,\n",
    "    load_preds,\n",
    "    make_query_from_dict,\n",
    "    process_rdf,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "enable_logging()\n",
    "\n",
    "\n",
    "ratings = pd.read_json(\n",
    "    \"../parquets/reasoning-ratings/template-2-reasoning-judge-responses.jsonl\",\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "\n",
    "rdf = load_preds(parquets_dir=\"../parquets\")\n",
    "\n",
    "\n",
    "def preprocess(rdf: pd.DataFrame, model_id=\"Qwen/Qwen3-32B\") -> pd.DataFrame:\n",
    "    metadata = {\n",
    "        \"template_id\": 31,\n",
    "        \"model_id\": model_id,\n",
    "        \"gen_kwargs\": \"set2\",\n",
    "        \"dataset\": \"CSC\",\n",
    "        \"judge_model_id\": \"gemini-2.5-pro\",\n",
    "    }\n",
    "    query = make_query_from_dict(metadata, rdf.columns)\n",
    "    rdf = rdf.query(query)\n",
    "    rdf = process_rdf(rdf)\n",
    "    rdf = join_correct_responses(rdf)\n",
    "    rdf = assign_cols_perf_metrics(rdf)\n",
    "    return rdf\n",
    "\n",
    "\n",
    "rdf = preprocess(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_repair\n",
    "from lewidi_lib import drop_failed_rows, drop_na_response_rows\n",
    "import numpy as np\n",
    "from prm800k import extract_rating\n",
    "\n",
    "ratings = drop_failed_rows(ratings)\n",
    "ratings[\"step_ratings\"] = (\n",
    "    ratings[\"response\"].apply(json_repair.loads).apply(extract_rating)\n",
    ")\n",
    "ratings = drop_na_response_rows(ratings, col=\"step_ratings\")\n",
    "ratings[\"avg_step_rating\"] = ratings[\"step_ratings\"].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_cols = [\"dataset\", \"dataset_idx\", \"run_idx\"]  # expand when more cols!\n",
    "ratings_cols = [\n",
    "    \"step_ratings\",\n",
    "    \"avg_step_rating\",\n",
    "    \"reasoning\",\n",
    "    \"judge_model_id\",\n",
    "    \"dataset\",\n",
    "    \"dataset_idx\",\n",
    "    \"run_idx\",\n",
    "]\n",
    "joint = ratings[ratings_cols].merge(\n",
    "    rdf, on=join_cols, how=\"inner\", suffixes=(\"_judge\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint[[\"avg_step_rating\", \"ws_loss\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### example 8: too lax with spread out distribution\n",
    "for k, v in (\n",
    "    joint.iloc[37][\n",
    "        [\"text\", \"reasoning\", \"response\", \"target\", \"reasoning_judge\", \"response_judge\"]\n",
    "    ]\n",
    "    .to_dict()\n",
    "    .items()\n",
    "):\n",
    "    print(k, v)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Using JointGrid directly for more control\n",
    "fgrid = sns.JointGrid(data=joint, x=\"avg_step_rating\", y=\"ws_loss\")\n",
    "fgrid.plot_joint(sns.scatterplot, alpha=0.5)\n",
    "fgrid.plot_joint(sns.regplot, scatter=False)  # Add regression line\n",
    "fgrid.plot_marginals(sns.histplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(joint.groupby(\"dataset_idx\").size() == 10).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdflarge = load_preds(\"/mnt/disk16tb/globus_shared/from-lrz-ai-systems/\")\n",
    "rdflarge = preprocess(rdflarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is almost no performance difference between the normal outputs\n",
    "# and those selected for top trace ratings\n",
    "avg_ws_loss = rdflarge.groupby(\"dataset_idx\", as_index=False).agg(\n",
    "    ws_loss=(\"ws_loss\", \"mean\"), pred_entropy=(\"pred_entropy\", \"mean\")\n",
    ")\n",
    "avg_ws_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_by_judge = joint.loc[joint.groupby(\"dataset_idx\")[\"avg_step_rating\"].idxmax()][\n",
    "    [\n",
    "        \"dataset_idx\",\n",
    "        \"avg_step_rating\",\n",
    "        \"tgt_has_holes\",\n",
    "        \"ws_loss\",\n",
    "        \"pred_entropy\",\n",
    "        \"target_entropy\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lewidi_lib import (\n",
    "    agg_perf_metrics,\n",
    "    compute_average_baseline,\n",
    "    process_rdf_and_add_perf_metrics,\n",
    ")\n",
    "\n",
    "model_avg_baseline = compute_average_baseline(rdflarge)\n",
    "gemini_raw = (\n",
    "    load_preds(\"../parquets/baseline\")\n",
    "    .query(\"template_id == 31\")\n",
    "    .pipe(process_rdf_and_add_perf_metrics)\n",
    ")\n",
    "gemini_agg = agg_perf_metrics(gemini_raw)\n",
    "gemini_model_avg = agg_perf_metrics(compute_average_baseline(gemini_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdflarge = rdflarge.assign(\n",
    "    entropy_rank=rdflarge.groupby(\"dataset_idx\")[\"pred_entropy\"]\n",
    "    .rank(method=\"first\")\n",
    "    .astype(int)\n",
    ")\n",
    "by_entropy = rdflarge.groupby(\"entropy_rank\", as_index=False)[\n",
    "    [\"ws_loss\", \"pred_entropy\"]\n",
    "].mean()\n",
    "by_entropy[\"type\"] = (\n",
    "    \"entropy\"  # \"entropy r\" + (by_entropy[\"entropy_rank\"] - 1).astype(str)\n",
    ")\n",
    "by_entropy.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"pred_entropy\", \"ws_loss\"]\n",
    "loss_vs_entropy = pd.DataFrame(\n",
    "    {\n",
    "        \"best_by_judge\": best_by_judge[cols].mean(),\n",
    "        \"simple\": avg_ws_loss[cols].mean(),\n",
    "        \"model_avg_baseline\": model_avg_baseline[cols].mean(),\n",
    "        \"gemini-2.5-pro\": gemini_agg[cols].mean(),\n",
    "        \"gemini-2.5-pro-model-avg\": gemini_model_avg[cols].mean(),\n",
    "    }\n",
    ").T.reset_index(names=\"type\")\n",
    "loss_vs_entropy = pd.concat([loss_vs_entropy, by_entropy.drop(columns=\"entropy_rank\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "grid = sns.JointGrid(data=loss_vs_entropy, x=\"pred_entropy\", y=\"ws_loss\")\n",
    "grid.plot_joint(sns.scatterplot, hue=loss_vs_entropy[\"type\"], style=loss_vs_entropy[\"type\"])\n",
    "grid.plot_marginals(sns.histplot, multiple=\"stack\")\n",
    "grid.ax_joint.legend(bbox_to_anchor=(1.2, 1), loc=\"upper left\")\n",
    "grid.ax_joint.grid(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "grid1 = sns.jointplot(data=model_avg_baseline, x=\"pred_entropy\", y=\"ws_loss\", marginal_kws={'bins': 20})\n",
    "grid1.fig.suptitle(\"Model Avg Baseline\", y=1.02)\n",
    "ylim = (rdflarge[\"ws_loss\"].min(), rdflarge[\"ws_loss\"].max())\n",
    "grid1.ax_joint.set_ylim(ylim)\n",
    "\n",
    "grid2 = sns.jointplot(data=rdflarge, x=\"pred_entropy\", y=\"ws_loss\", marginal_kws={'bins': 20})\n",
    "grid2.fig.suptitle(\"Simple\", y=1.02)\n",
    "grid1.ax_joint.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data = pd.concat(\n",
    "    [\n",
    "        rdflarge.assign(type=\"simple\"),\n",
    "        model_avg_baseline.assign(type=\"model_avg_baseline\"),\n",
    "        best_by_judge.assign(type=\"best_by_judge\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "sns.violinplot(hist_data, x=\"type\", y=\"ws_loss\", common_norm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tail of High Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_by_judge.sort_values(\"ws_loss\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(best_by_judge, x=\"pred_entropy\", y=\"ws_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    joint.query(\"dataset_idx == 6827\"),\n",
    "    x=\"avg_step_rating\",\n",
    "    y=\"ws_loss\",\n",
    "    hue=\"pred_entropy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ratings = joint.groupby(\"dataset_idx\", as_index=False).agg(\n",
    "    n_max_ratings=(\"avg_step_rating\", lambda s: (s.max() == s).sum()),\n",
    "    max_rating=(\"avg_step_rating\", \"max\"),\n",
    "    avg_pred_entropy=(\"pred_entropy\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(max_ratings.merge(best_by_judge), x=\"n_max_ratings\", y=\"avg_pred_entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When does Model Averaging improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvements = (\n",
    "    rdflarge.groupby([\"dataset_idx\", \"tgt_has_holes\"], as_index=False)\n",
    "    .agg(\n",
    "        avg_ws_loss=(\"ws_loss\", \"mean\"),\n",
    "        avg_pred_entropy=(\"pred_entropy\", \"mean\"),\n",
    "    )\n",
    "    .merge(model_avg_baseline[[\"dataset_idx\", \"ws_loss\", \"pred_entropy\"]])\n",
    "    .assign(improvement=lambda df: df[\"avg_ws_loss\"] - df[\"ws_loss\"])\n",
    "    .assign(pred_entropy_diff=lambda df: df[\"pred_entropy\"] - df[\"avg_pred_entropy\"])\n",
    ")\n",
    "\n",
    "improvements.sort_values(\"improvement\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    improvements,\n",
    "    x=\"improvement\",\n",
    "    y=\"pred_entropy_diff\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(improvements, x=\"improvement\", hue=\"tgt_has_holes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = rdflarge.query(\"dataset_idx == 2092\").sort_values(\"ws_loss\")\n",
    "example[[\"pred\", \"ws_loss\"]].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[\"ws_loss\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_improved = compute_average_baseline(example)\n",
    "print(example_improved[\"ws_loss\"])\n",
    "sns.barplot(example_improved[\"target\"].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
