{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lewidi_lib import (\n",
    "    assign_cols_perf_metrics,\n",
    "    enable_logging,\n",
    "    join_correct_responses,\n",
    "    load_preds,\n",
    "    make_query_from_dict,\n",
    "    process_rdf,\n",
    ")\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "enable_logging()\n",
    "\n",
    "\n",
    "# ratings = pd.read_json(\n",
    "#     \"../parquets/reasoning-ratings/template-2-reasoning-judge-responses.jsonl\",\n",
    "#     lines=True,\n",
    "# )\n",
    "ratings = pd.read_parquet(\n",
    "    \"/home/tomasruiz/datasets/dss_home/lewidi-data/sbatch/di38bec/Qwen_Qwen3-8B/set2/t31/allexs_20loops/judge/Qwen/Qwen3-32B/set2/t3/1000exs_10loops/responses.parquet\",\n",
    ")\n",
    "if \"split\" not in ratings.columns:\n",
    "    ratings = ratings.assign(split=\"train\")\n",
    "print(\"len(ratings)=\", len(ratings))\n",
    "\n",
    "rdf = load_preds(parquets_dir=\"../parquets\")\n",
    "rdf = load_preds(\n",
    "    # \"/home/tomasruiz/datasets/dss_home/lewidi-data/sbatch/di38bec/tasks_0_cscfull_t31_Qwen_Qwen3-32B_set2/preds\"\n",
    "    \"/home/tomasruiz/datasets/dss_home/lewidi-data/sbatch/di38bec/Qwen_Qwen3-8B/set2/t31/allexs_20loops/preds\"\n",
    ")\n",
    "rdf.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "def preprocess(rdf: pd.DataFrame, model_id=\"Qwen/Qwen3-32B\") -> pd.DataFrame:\n",
    "    metadata = {\n",
    "        \"template_id\": 31,\n",
    "        # \"model_id\": model_id,\n",
    "        \"gen_kwargs\": \"set2\",\n",
    "        \"dataset\": \"CSC\",\n",
    "        \"judge_model_id\": \"gemini-2.5-pro\",\n",
    "    }\n",
    "    query = make_query_from_dict(metadata, rdf.columns)\n",
    "    rdf = rdf.query(query)\n",
    "    rdf = rdf.query(\"run_idx.isin([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\")\n",
    "    rdf = process_rdf(rdf)\n",
    "    rdf = join_correct_responses(rdf)\n",
    "    rdf = assign_cols_perf_metrics(rdf)\n",
    "    return rdf\n",
    "\n",
    "\n",
    "rdf = preprocess(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lewidi_lib import (\n",
    "    assign_col_score_from_scalar,\n",
    "    assing_col_score_from_json,\n",
    "    create_rating_matrix,\n",
    "    drop_failed_rows,\n",
    "    drop_na_response_rows,\n",
    ")\n",
    "\n",
    "ratings = drop_failed_rows(ratings)\n",
    "ratings = drop_na_response_rows(ratings)\n",
    "\n",
    "use_json_ratings = False\n",
    "if use_json_ratings:\n",
    "    ratings = assing_col_score_from_json(ratings)\n",
    "else:\n",
    "    ratings = assign_col_score_from_scalar(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_cols = [\"dataset\", \"dataset_idx\", \"run_idx\"]  # expand when more cols!\n",
    "ratings_cols = [\n",
    "    \"response\",\n",
    "    # \"prompt\",\n",
    "    \"step_ratings\",\n",
    "    \"score\",\n",
    "    \"reasoning\",\n",
    "    \"judge_model_id\",\n",
    "    \"dataset\",\n",
    "    \"split\",\n",
    "    \"dataset_idx\",\n",
    "    \"run_idx\",\n",
    "]\n",
    "\n",
    "\n",
    "def join_ratings(rdf: pd.DataFrame, ratings: pd.DataFrame, ratings_cols=ratings_cols):\n",
    "    return ratings[ratings_cols].merge(\n",
    "        rdf, on=join_cols, how=\"inner\", suffixes=(\"_judge\", \"\")\n",
    "    )\n",
    "\n",
    "\n",
    "joint = join_ratings(rdf, ratings, ratings_cols=ratings.columns)\n",
    "joint = joint.assign(row_idx=range(len(joint)))\n",
    "joint = joint.assign(\n",
    "    score_rank=joint.groupby(\"dataset_idx\")[\"score\"].rank(method=\"first\").astype(\"int\")\n",
    ")\n",
    "# assert len(joint) == len(ratings), (len(joint), len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_json_ratings:\n",
    "    all_best_rows = create_rating_matrix(ratings)\n",
    "    join_ratings(\n",
    "        rdf, all_best_rows, ratings_cols=[*ratings_cols, \"rating_type\", \"reduction\"]\n",
    "    ).groupby([\"reduction\", \"rating_type\"]).agg(\n",
    "        score=(\"score\", \"mean\"),\n",
    "        ws_loss=(\"ws_loss\", \"mean\"),\n",
    "        # pred_entropy=(\"pred_entropy\", \"mean\"),\n",
    "    ).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = []\n",
    "for dataset_idx, group in joint.groupby(\"dataset_idx\"):\n",
    "    corrs.append(group[[\"score\", \"ws_loss\"]].corr()[\"score\"][\"ws_loss\"])\n",
    "pd.Series(corrs).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint[[\"score\", \"ws_loss\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_by_judge_idxs = joint.groupby(\"dataset_idx\")[\"score\"].idxmax()\n",
    "best_by_judge = joint.loc[best_by_judge_idxs][\n",
    "    [\n",
    "        \"dataset_idx\",\n",
    "        \"score\",\n",
    "        \"tgt_has_holes\",\n",
    "        \"ws_loss\",\n",
    "        \"pred_entropy\",\n",
    "        \"target_entropy\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "joint.groupby(\"score_rank\")[[\"score\", \"ws_loss\"]].mean()\n",
    "sns.lineplot(joint, x=\"score_rank\", y=\"ws_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lewidi_lib import compute_average_baseline\n",
    "\n",
    "worst_by_judge_idxs = joint.groupby(\"dataset_idx\")[\"score\"].idxmin()\n",
    "worst_by_judge = joint.loc[worst_by_judge_idxs]\n",
    "discard_worst = joint.query(\"~row_idx.isin(@worst_by_judge['row_idx'])\")\n",
    "discard_worst_model_avg = compute_average_baseline(discard_worst)\n",
    "(\n",
    "    worst_by_judge[\"ws_loss\"].mean().round(3),\n",
    "    discard_worst[\"ws_loss\"].mean().round(3),\n",
    "    discard_worst_model_avg[\"ws_loss\"].mean().round(3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint.loc[best_by_judge_idxs, \"judge_rating\"] = \"best\"\n",
    "joint.loc[worst_by_judge_idxs, \"judge_rating\"] = \"worst\"\n",
    "joint = joint.assign(judge_rating=joint[\"judge_rating\"].fillna(\"in-between\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "col_wrap = 6\n",
    "n_cols = 4\n",
    "dataset_idxs = joint[\"dataset_idx\"].unique()[: col_wrap * n_cols]\n",
    "\n",
    "fgrid = sns.FacetGrid(\n",
    "    joint.query(\"dataset_idx.isin(@dataset_idxs)\"),\n",
    "    col=\"dataset_idx\",\n",
    "    col_wrap=col_wrap,\n",
    "    height=3,\n",
    "    aspect=1,\n",
    ")\n",
    "fgrid.map_dataframe(\n",
    "    sns.scatterplot, x=\"score\", y=\"ws_loss\", hue=\"best_by_judge\", alpha=0.5\n",
    ")\n",
    "fgrid.map_dataframe(\n",
    "    sns.regplot, x=\"score\", y=\"ws_loss\", scatter=False, color=\"steelblue\"\n",
    ")\n",
    "\n",
    "for ax in fgrid.axes.flat:\n",
    "    ax.grid(alpha=0.5)\n",
    "fgrid.add_legend(title=\"Best by judge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Using JointGrid directly for more control\n",
    "fgrid = sns.JointGrid(data=joint, x=\"score\", y=\"ws_loss\")\n",
    "fgrid.plot_joint(sns.scatterplot, data=joint, alpha=0.5, hue=\"judge_rating\")\n",
    "fgrid.plot_joint(sns.regplot, scatter=False)  # Add regression line\n",
    "fgrid.plot_marginals(\n",
    "    sns.histplot, data=joint, hue=\"judge_rating\", stat=\"density\", common_norm=False\n",
    ")\n",
    "fgrid.ax_joint.legend(bbox_to_anchor=(1.2, 1), loc=\"upper left\", title=\"Judge Rating\")\n",
    "fgrid.ax_joint.grid(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "\n",
    "joint = joint.assign(reasoning_len_chars=joint[\"reasoning\"].apply(len))\n",
    "most_cot_chars = joint.loc[joint.groupby(\"dataset_idx\")[\"reasoning_len_chars\"].idxmax()]\n",
    "\n",
    "joint = joint.assign(\n",
    "    reasoning_len_steps=joint[\"reasoning\"].apply(lambda r: len(nltk.sent_tokenize(r)))\n",
    ")\n",
    "most_cot_steps = joint.loc[joint.groupby(\"dataset_idx\")[\"reasoning_len_steps\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lewidi_lib import (\n",
    "    agg_perf_metrics,\n",
    "    compute_average_baseline,\n",
    "    process_rdf_and_add_perf_metrics,\n",
    ")\n",
    "\n",
    "model_avg_baseline = compute_average_baseline(joint)\n",
    "gemini_raw = (\n",
    "    load_preds(\"../parquets/baseline\")\n",
    "    .query(\"template_id == 31\")\n",
    "    .pipe(process_rdf_and_add_perf_metrics)\n",
    ")\n",
    "gemini_agg = agg_perf_metrics(gemini_raw)\n",
    "gemini_model_avg = agg_perf_metrics(compute_average_baseline(gemini_raw))\n",
    "gemini_agg.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model_avg.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = joint.assign(\n",
    "    entropy_rank=joint.groupby(\"dataset_idx\")[\"pred_entropy\"]\n",
    "    .rank(method=\"first\")\n",
    "    .astype(int)\n",
    ")\n",
    "by_entropy = joint.groupby(\"entropy_rank\", as_index=False)[\n",
    "    [\"ws_loss\", \"pred_entropy\"]\n",
    "].mean()\n",
    "by_entropy[\"type\"] = (\n",
    "    \"entropy\"  # \"entropy r\" + (by_entropy[\"entropy_rank\"] - 1).astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do we have enough examples to judge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "subset = rdf.merge(\n",
    "    joint[[\"dataset_idx\", \"run_idx\"]], on=[\"dataset_idx\", \"run_idx\"], how=\"inner\"\n",
    ")\n",
    "# assert len(subset) == len(joint)\n",
    "# assert np.isclose(joint[\"ws_loss\"].mean(), subset[\"ws_loss\"].mean())\n",
    "joint[\"ws_loss\"].mean().round(3), rdf[\"ws_loss\"].mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"pred_entropy\", \"ws_loss\"]\n",
    "loss_vs_entropy = pd.DataFrame(\n",
    "    {\n",
    "        \"best_by_judge\": best_by_judge[cols].mean(),\n",
    "        \"discard_worst_by_judge\": discard_worst[cols].mean(),\n",
    "        \"simple (full dataset)\": rdf[cols].mean(),\n",
    "        \"simple (judged subset)\": joint[cols].mean(),\n",
    "        \"model-avg (judged subset)\": model_avg_baseline[cols].mean(),\n",
    "        \"model-avg (full dataset)\": compute_average_baseline(rdf)[cols].mean(),\n",
    "        \"gemini-2.5-pro\": gemini_agg.query(\"model_id.str.contains('gemini-2.5-pro')\")[\n",
    "            cols\n",
    "        ].mean(),\n",
    "        \"gemini-2.5-pro-model-avg\": gemini_model_avg.query(\n",
    "            \"model_id.str.contains('gemini-2.5-pro')\"\n",
    "        )[cols].mean(),\n",
    "        # \"most_cot_chars\": most_cot_chars[cols].mean(),\n",
    "        # \"most_cot_steps\": most_cot_steps[cols].mean(),\n",
    "    }\n",
    ").T.reset_index(names=\"type\")\n",
    "loss_vs_entropy = pd.concat([loss_vs_entropy, by_entropy.drop(columns=\"entropy_rank\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "grid = sns.JointGrid(data=loss_vs_entropy, x=\"pred_entropy\", y=\"ws_loss\")\n",
    "grid.plot_joint(\n",
    "    sns.scatterplot, hue=loss_vs_entropy[\"type\"], style=loss_vs_entropy[\"type\"]\n",
    ")\n",
    "grid.plot_marginals(sns.histplot, multiple=\"stack\")\n",
    "grid.ax_joint.legend(bbox_to_anchor=(1.2, 1), loc=\"upper left\")\n",
    "grid.ax_joint.grid(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"dataset_idx\"].nunique(), ratings[\"run_idx\"].nunique(), len(ratings.drop_duplicates(subset=[\"dataset_idx\", \"run_idx\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vs_entropy.head(10).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf[\"dataset_idx\"].nunique(), rdf[\"run_idx\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Mean Variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from lewidi_lib import bootstrap_avg\n",
    "\n",
    "losses_by_example = joint.groupby(\"dataset_idx\", as_index=False)[\"ws_loss\"].mean()\n",
    "low, mean, high = bootstrap_avg(losses_by_example[\"ws_loss\"])\n",
    "print(f\"Mean: {mean:.3f}, 95% CI: {low:.3f} - {high:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full_losses_by_example = (\n",
    "    rdf.query(\"run_idx < 10\").groupby(\"dataset_idx\", as_index=False)[\"ws_loss\"].mean()\n",
    ")\n",
    "low, mean, high = bootstrap_avg(full_losses_by_example[\"ws_loss\"])\n",
    "print(f\"Mean: {mean:.3f}, 95% CI: {low:.3f} - {high:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from lewidi_lib import get_stable_random_subset\n",
    "\n",
    "all_res = []\n",
    "n_samples = [100, 300, 500, 1000, 2000, 3000, 4000, 5000]\n",
    "for n in n_samples:\n",
    "    ds_idxs = get_stable_random_subset(rdf[\"dataset_idx\"], n)\n",
    "    subset = rdf.query(\"dataset_idx in @ds_idxs\")\n",
    "    subset_losses_by_example = subset.groupby(\"dataset_idx\", as_index=False)[\"ws_loss\"].mean()\n",
    "    res = bootstrap_avg(subset_losses_by_example[\"ws_loss\"])\n",
    "    all_res.append(res)\n",
    "    low, mean, high = res\n",
    "    print(f\"#Examples: {n}, Mean: {mean:.2f}, 95% CI: {low:.2f} - {high:.2f}, CI width: {high - low:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
