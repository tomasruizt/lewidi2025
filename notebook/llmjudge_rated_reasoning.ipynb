{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lewidi_lib import (\n",
    "    assign_cols_perf_metrics,\n",
    "    enable_logging,\n",
    "    join_correct_responses,\n",
    "    load_preds,\n",
    "    make_query_from_dict,\n",
    "    process_rdf,\n",
    ")\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "enable_logging()\n",
    "\n",
    "\n",
    "ratings = pd.read_json(\n",
    "    \"../parquets/reasoning-ratings/template-2-reasoning-judge-responses.jsonl\",\n",
    "    # \"/mnt/disk16tb/globus_shared/from-lrz-ai-systems/tasks_0_cscfull_t31_Qwen_Qwen3-32B_set2/judge/Qwen3-32B.jsonl\",\n",
    "    lines=True,\n",
    ")\n",
    "if \"split\" not in ratings.columns:\n",
    "    ratings = ratings.assign(split=\"train\")\n",
    "print(\"len(ratings)=\", len(ratings))\n",
    "\n",
    "\n",
    "rdf = load_preds(parquets_dir=\"../parquets\")\n",
    "# rdf = load_preds(\n",
    "#     parquets_dir=\"/mnt/disk16tb/globus_shared/from-lrz-ai-systems/tasks_0_cscfull_t31_Qwen_Qwen3-32B_set2/preds\"\n",
    "# )\n",
    "rdf.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "def preprocess(rdf: pd.DataFrame, model_id=\"Qwen/Qwen3-32B\") -> pd.DataFrame:\n",
    "    metadata = {\n",
    "        \"template_id\": 31,\n",
    "        \"model_id\": model_id,\n",
    "        \"gen_kwargs\": \"set2\",\n",
    "        \"dataset\": \"CSC\",\n",
    "        \"judge_model_id\": \"gemini-2.5-pro\",\n",
    "    }\n",
    "    query = make_query_from_dict(metadata, rdf.columns)\n",
    "    rdf = rdf.query(query)\n",
    "    rdf = process_rdf(rdf)\n",
    "    rdf = join_correct_responses(rdf)\n",
    "    rdf = assign_cols_perf_metrics(rdf)\n",
    "    return rdf\n",
    "\n",
    "\n",
    "rdf = preprocess(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import json_repair\n",
    "from lewidi_lib import (\n",
    "    create_rating_matrix,\n",
    "    drop_failed_rows,\n",
    "    drop_na_response_rows,\n",
    "    process_ratings,\n",
    ")\n",
    "from prm800k import mapping\n",
    "\n",
    "ratings = drop_failed_rows(ratings)\n",
    "ratings = drop_na_response_rows(ratings)\n",
    "ratings[\"response_parsed\"] = ratings[\"response\"].apply(json_repair.loads)\n",
    "ratings = process_ratings(ratings, cat_mapping=mapping(ok=0, bad=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_cols = [\"dataset\", \"dataset_idx\", \"run_idx\"]  # expand when more cols!\n",
    "ratings_cols = [\n",
    "    \"response\",\n",
    "    \"step_ratings\",\n",
    "    \"score\",\n",
    "    \"reasoning\",\n",
    "    \"judge_model_id\",\n",
    "    \"dataset\",\n",
    "    \"split\",\n",
    "    \"dataset_idx\",\n",
    "    \"run_idx\",\n",
    "]\n",
    "\n",
    "\n",
    "def join_ratings(rdf: pd.DataFrame, ratings: pd.DataFrame, ratings_cols=ratings_cols):\n",
    "    return ratings[ratings_cols].merge(\n",
    "        rdf, on=join_cols, how=\"inner\", suffixes=(\"_judge\", \"\")\n",
    "    )\n",
    "\n",
    "\n",
    "joint = join_ratings(rdf, ratings)\n",
    "# assert len(joint) == len(ratings), (len(joint), len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_rows = create_rating_matrix(ratings)\n",
    "join_ratings(\n",
    "    rdf, all_best_rows, ratings_cols=[*ratings_cols, \"rating_type\", \"reduction\"]\n",
    ").groupby([\"reduction\", \"rating_type\"]).agg(\n",
    "    score=(\"score\", \"mean\"),\n",
    "    ws_loss=(\"ws_loss\", \"mean\"),\n",
    "    # pred_entropy=(\"pred_entropy\", \"mean\"),\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint[[\"score\", \"ws_loss\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Using JointGrid directly for more control\n",
    "fgrid = sns.JointGrid(data=joint, x=\"score\", y=\"ws_loss\")\n",
    "fgrid.plot_joint(sns.scatterplot, alpha=0.5)\n",
    "fgrid.plot_joint(sns.regplot, scatter=False)  # Add regression line\n",
    "fgrid.plot_marginals(sns.histplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(joint.groupby(\"dataset_idx\").size() == 10).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdflarge = load_preds(\n",
    "    \"/mnt/disk16tb/globus_shared/from-lrz-ai-systems/tasks_0_cscfull_t31_Qwen_Qwen3-32B_set2/preds\"\n",
    ")\n",
    "rdflarge.drop_duplicates(inplace=True)\n",
    "rdflarge = preprocess(rdflarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is almost no performance difference between the normal outputs\n",
    "# and those selected for top trace ratings\n",
    "avg_ws_loss = rdflarge.groupby(\"dataset_idx\", as_index=False).agg(\n",
    "    ws_loss=(\"ws_loss\", \"mean\"), pred_entropy=(\"pred_entropy\", \"mean\")\n",
    ")\n",
    "avg_ws_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_by_judge = joint.loc[joint.groupby(\"dataset_idx\")[\"score\"].idxmax()][\n",
    "    [\n",
    "        \"dataset_idx\",\n",
    "        \"score\",\n",
    "        \"tgt_has_holes\",\n",
    "        \"ws_loss\",\n",
    "        \"pred_entropy\",\n",
    "        \"target_entropy\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdflarge = rdflarge.assign(reasoning_len_chars=rdflarge[\"reasoning\"].apply(len))\n",
    "most_cot_chars = rdflarge.loc[\n",
    "    rdflarge.groupby(\"dataset_idx\")[\"reasoning_len_chars\"].idxmax()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "fst_n_dataset_idxs = rdflarge[\"dataset_idx\"].unique()[:100]\n",
    "rdflarge_n_ixs = rdflarge.query(\"dataset_idx.isin(@fst_n_dataset_idxs)\")\n",
    "rdflarge_n_ixs = rdflarge_n_ixs.assign(\n",
    "    reasoning_len_steps=rdflarge_n_ixs[\"reasoning\"].apply(\n",
    "        lambda r: len(nltk.sent_tokenize(r))\n",
    "    )\n",
    ")\n",
    "most_cot_steps = rdflarge_n_ixs.loc[\n",
    "    rdflarge_n_ixs.groupby(\"dataset_idx\")[\"reasoning_len_steps\"].idxmax()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lewidi_lib import (\n",
    "    agg_perf_metrics,\n",
    "    compute_average_baseline,\n",
    "    process_rdf_and_add_perf_metrics,\n",
    ")\n",
    "\n",
    "model_avg_baseline = compute_average_baseline(rdflarge)\n",
    "gemini_raw = (\n",
    "    load_preds(\"../parquets/baseline\")\n",
    "    .query(\"template_id == 31\")\n",
    "    .pipe(process_rdf_and_add_perf_metrics)\n",
    ")\n",
    "gemini_agg = agg_perf_metrics(gemini_raw)\n",
    "gemini_model_avg = agg_perf_metrics(compute_average_baseline(gemini_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdflarge = rdflarge.assign(\n",
    "    entropy_rank=rdflarge.groupby(\"dataset_idx\")[\"pred_entropy\"]\n",
    "    .rank(method=\"first\")\n",
    "    .astype(int)\n",
    ")\n",
    "by_entropy = rdflarge.groupby(\"entropy_rank\", as_index=False)[\n",
    "    [\"ws_loss\", \"pred_entropy\"]\n",
    "].mean()\n",
    "by_entropy[\"type\"] = (\n",
    "    \"entropy\"  # \"entropy r\" + (by_entropy[\"entropy_rank\"] - 1).astype(str)\n",
    ")\n",
    "by_entropy.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"pred_entropy\", \"ws_loss\"]\n",
    "loss_vs_entropy = pd.DataFrame(\n",
    "    {\n",
    "        \"best_by_judge\": best_by_judge[cols].mean(),\n",
    "        \"simple\": avg_ws_loss[cols].mean(),\n",
    "        \"model_avg_baseline\": model_avg_baseline[cols].mean(),\n",
    "        \"gemini-2.5-pro\": gemini_agg[cols].mean(),\n",
    "        \"gemini-2.5-pro-model-avg\": gemini_model_avg[cols].mean(),\n",
    "        \"most_cot_chars\": most_cot_chars[cols].mean(),\n",
    "        \"most_cot_steps\": most_cot_steps[cols].mean(),\n",
    "    }\n",
    ").T.reset_index(names=\"type\")\n",
    "loss_vs_entropy = pd.concat([loss_vs_entropy, by_entropy.drop(columns=\"entropy_rank\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "grid = sns.JointGrid(data=loss_vs_entropy, x=\"pred_entropy\", y=\"ws_loss\")\n",
    "grid.plot_joint(\n",
    "    sns.scatterplot, hue=loss_vs_entropy[\"type\"], style=loss_vs_entropy[\"type\"]\n",
    ")\n",
    "grid.plot_marginals(sns.histplot, multiple=\"stack\")\n",
    "grid.ax_joint.legend(bbox_to_anchor=(1.2, 1), loc=\"upper left\")\n",
    "grid.ax_joint.grid(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data = pd.concat(\n",
    "    [\n",
    "        rdflarge.assign(type=\"simple\"),\n",
    "        model_avg_baseline.assign(type=\"model_avg_baseline\"),\n",
    "        best_by_judge.assign(type=\"best_by_judge\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "sns.violinplot(hist_data, x=\"type\", y=\"ws_loss\", common_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(best_by_judge, x=\"pred_entropy\", y=\"ws_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(\n",
    "    joint.query(\"dataset_idx == dataset_idx.unique()[0]\"),\n",
    "    x=\"pred_entropy\",\n",
    "    y=\"ws_loss\",\n",
    "    hue=\"score\",\n",
    ")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
