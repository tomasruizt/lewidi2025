{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from lewidi_lib import enable_logging\n",
    "\n",
    "enable_logging()\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prm800k import load_prm800k_phase2_dataset\n",
    "\n",
    "\n",
    "dataset = load_prm800k_phase2_dataset(split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "from prm800k import problems_with_50pct_correct_solutions\n",
    "\n",
    "\n",
    "half_correct = problems_with_50pct_correct_solutions(dataset, n_problem_ids=10)\n",
    "\n",
    "sns.lmplot(half_correct, x=\"avg_rating\", y=\"correct\", logistic=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Gemini Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_repair\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "mapping = {\"great\": 1, \"ok\": 0, \"okay\": 0, \"bad\": -1}\n",
    "\n",
    "gdf = pd.read_json(\"./gemini-prm800k-ratings.jsonl\", lines=True)\n",
    "logger.info(\"Dropping %d rows with success=False\", len(gdf.query(\"not success\")))\n",
    "gdf = gdf.query(\"success\")\n",
    "gdf[\"ratings\"] = (\n",
    "    gdf[\"response\"]\n",
    "    .apply(json_repair.loads)\n",
    "    .apply(lambda x: [mapping[r[\"rating\"]] for r in x])\n",
    ")\n",
    "gdf[\"avg_rating\"] = gdf[\"ratings\"].apply(np.mean)\n",
    "gdf[\"n_ratings\"] = gdf[\"ratings\"].apply(len)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = dataset.merge(gdf, on=\"dataset_idx\", suffixes=(\"_humans\", \"_gemini\"))\n",
    "joined[\"equal_n_ratings\"] = joined[\"n_ratings_humans\"] == joined[\"n_ratings_gemini\"]\n",
    "logger.info(\n",
    "    \"Dropping %d rows with unequal n_ratings\", len(joined.query(\"not equal_n_ratings\"))\n",
    ")\n",
    "joined = joined.query(\"equal_n_ratings\")\n",
    "joined.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for _, row in joined.query(\"~equal_n_ratings\")[[\"texts\", \"response\"]].iterrows():\n",
    "    print(json.dumps(row[\"texts\"], indent=2))\n",
    "    print(row[\"response\"])\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"ratings_humans\", \"ratings_gemini\"]\n",
    "comparison = joined[cols].explode(column=cols).astype(int).reset_index(drop=True)\n",
    "comparison.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(comparison[\"ratings_gemini\"] == comparison[\"ratings_humans\"]).mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff\n",
    "\n",
    "krippendorff.alpha(reliability_data=comparison.values.T).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Annotator Agreement\n",
    "This cannot be computed, because the different annotators received different LLM CoTs to rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from prm800k import load_raw_prm800k_phase2_dataset\n",
    "\n",
    "\n",
    "prm800ktrain = load_raw_prm800k_phase2_dataset(split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prm800ktrain.query(\"is_quality_control_question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini BoN Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    joined.groupby(\"problem_id\", as_index=False)\n",
    "    .agg(avg_correct=(\"correct\", \"mean\"), avg_rating_humans=(\"avg_rating_humans\", \"mean\"), avg_rating_gemini=(\"avg_rating_gemini\", \"mean\"))\n",
    "    .round(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoN based on Gemini ratings is pretty good!\n",
    "joined.loc[joined.groupby(\"problem_id\")[\"avg_rating_gemini\"].idxmax()][\"correct\"].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(\n",
    "    joined, \n",
    "    x=\"avg_rating_humans\", \n",
    "    y=\"avg_rating_gemini\", \n",
    "    color=\"correct\",\n",
    "    trendline=\"ols\",\n",
    "    title=\"Gemini vs Human Ratings by Correctness\",\n",
    "    hover_data=[\"dataset_idx\"],\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
